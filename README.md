
# ğŸ’¬ RAG Chatbot with Context Memory  
A conversational AI chatbot using Retrieval-Augmented Generation (RAG) with memory. Built with LangChain, OpenAI, FAISS, and Streamlit.  
  
## ğŸ§  Features  
- Context-aware Q&A from custom documents  
- Uses FAISS vector index for fast semantic search  
- Dual interface: Streamlit (frontend) + FastAPI (backend)  
- LangChain memory for conversational flow  
  
## ğŸ“ Project Structure  
```
rag-chatbot/  
â”œâ”€â”€ data/docs/              # Your document PDFs or text files  
â”œâ”€â”€ src/  
â”‚   â”œâ”€â”€ chatbot_app.py      # Streamlit UI  
â”‚   â””â”€â”€ api.py              # FastAPI backend  
â”œâ”€â”€ vector_store/           # FAISS index stored here  
â”œâ”€â”€ requirements.txt  
â”œâ”€â”€ LICENSE  
â”œâ”€â”€ README.md  
```  
  
## ğŸš€ How to Run  
  
### 1. Install dependencies  
```bash  
pip install -r requirements.txt  
```  
  
### 2. Start the chatbot (Streamlit UI)  
```bash  
streamlit run src/chatbot_app.py  
```  
  
### 3. Optional: Start backend API  
```bash  
uvicorn src.api:app --reload  
```  
  
## ğŸ“š Data  
- Store your PDFs or `.txt` files in `data/docs/`  
- FAISS index will be created in `vector_store/` after first run  
  
## ğŸ› ï¸ Tech Stack  
- Python, LangChain, OpenAI API  
- FAISS (vector DB), Streamlit  
- FastAPI (backend), PyPDF, Tiktoken  
